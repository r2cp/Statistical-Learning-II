{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR - *Forward propagation* y *backpropagation*\n",
    "\n",
    "Statistical Learning II\n",
    "\n",
    "Rodrigo Chang\n",
    "\n",
    "19000625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Librerías y funciones de activación y bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Activación de escalón\n",
    "def heaviside(x):\n",
    "    return (x >= 0).astype(np.float)\n",
    "\n",
    "# Función de activación ReLU\n",
    "def ReLU(x):\n",
    "    return np.maximum(0., x)\n",
    "\n",
    "def ReLU_gradient(x):\n",
    "    return (x > 0).astype(np.float)\n",
    "\n",
    "def linearAct(x):\n",
    "    return x\n",
    "\n",
    "# Función para añadir el término de bias a la primera fila\n",
    "# x tiene shape (n, k)\n",
    "# Devuelve matriz con tamaño (n+1, k)\n",
    "def addBias(x):\n",
    "    return np.vstack((np.ones(x.shape[1]), x))\n",
    "\n",
    "# Función para quitar el término de bias de la primera fila\n",
    "# x tiene shape (n+1, k)\n",
    "# Devuelve matriz con tamaño (n, k)\n",
    "def removeBias(x):\n",
    "    return x[1:, :].reshape(-1, x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894142, 0.5       , 0.73105858, 0.88079708])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([-1, 0., 1., 2.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLU(np.array([-1, 0., 1., 2.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLU_gradient(np.array([-1, 0., 1., 2.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Conjunto de datos de entrenamiento\n",
    "Definimos el conjunto de datos de entrenamiento. En este caso, queremos que la red neuronal aprenda la función XOR, por lo tanto, utilizaremos dos entradas y sus respectivas salidas conocidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain = np.array([0., 1., 1., 0.]).reshape(-1, 1)\n",
    "Ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1., 1.],\n",
       "        [0., 1., 0., 1.]]), array([[0., 1., 1., 0.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.T, Ytrain.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Matrices de pesos sinápticos\n",
    "Definimos los pesos sinápticos iniciales de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00720114, -0.06186034, -0.02669813],\n",
       "       [-0.05996283, -0.00296675,  0.06820584],\n",
       "       [ 0.1484713 ,  0.00988095,  0.04911111]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capa oculta 1\n",
    "k1 = 3\n",
    "Theta1 = np.random.normal(scale=0.1, size=(k1, 3))\n",
    "Theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09305555,  0.16179438,  0.168242  , -0.07952591],\n",
       "       [-0.02557751,  0.05143957, -0.11717614,  0.06511632]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capa oculta 2\n",
    "k2 = 2\n",
    "Theta2 = np.random.normal(scale=0.1, size=(k2, k1+1))\n",
    "Theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01624594, -0.12037319, -0.10486895]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capa de salida\n",
    "Theta3 = np.random.normal(scale=0.1, size=(1, 3))\n",
    "Theta3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## *Forward propagation* y cómputo de la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1., 1.],\n",
       "        [0., 1., 0., 1.]]), (2, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.T, Xtrain.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(X, Theta1, Theta2, Theta3, retHidden=False):\n",
    "    # Primera capa oculta\n",
    "    z_2 = np.matmul(Theta1, addBias(X))\n",
    "    a_2 = ReLU(z_2)\n",
    "    # Segunda capa oculta\n",
    "    z_3 = np.matmul(Theta2, addBias(a_2))\n",
    "    a_3 = ReLU(z_3)\n",
    "    # Capa de salida\n",
    "    z_4 = np.matmul(Theta3, addBias(a_3))\n",
    "    a_4 = linearAct(z_4)\n",
    "    \n",
    "    if retHidden:\n",
    "        return a_3, a_4\n",
    "    \n",
    "    return a_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00646584, 0.00676903, 0.00656042, 0.0069237 ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardProp(Xtrain.T, Theta1, Theta2, Theta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0224109]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardProp(np.array([100., 50.]).reshape(-1, 1), Theta1, Theta2, Theta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## *Backpropagation* de un ejemplo\n",
    "Propagación del error hacia atrás utilizando el ejemplo $(0, 1)$ cuya salida debería ser $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.],\n",
       "        [1.]]), array([[1.]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest = Xtrain[1, :].reshape(-1, 1)\n",
    "Ytest = Ytrain[1].reshape(-1,1)\n",
    "Xtest, Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99323097]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Propagamos hacia adelante el ejemplo\n",
    "a_1 = addBias(Xtest)\n",
    "# Primera capa oculta\n",
    "z_2 = np.matmul(Theta1, a_1)\n",
    "a_2 = ReLU(z_2)\n",
    "# Segunda capa oculta\n",
    "a_2 = addBias(a_2)\n",
    "z_3 = np.matmul(Theta2, a_2)\n",
    "a_3 = ReLU(z_3)\n",
    "# Capa de salida\n",
    "a_3 = addBias(a_3)\n",
    "z_4 = np.matmul(Theta3, a_3)\n",
    "a_4 = linearAct(z_4)\n",
    "\n",
    "## Backpropagation\n",
    "# Computamos el error en la capa de salida: d_4\n",
    "d_4 = a_4 - Ytest\n",
    "d_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11955838],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Propagamos hacia atrás\n",
    "d_3 = np.matmul(Theta3.T[1:], d_4) * ReLU_gradient(z_3)\n",
    "d_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.02011474],\n",
       "       [-0.00950799]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Propagamos hacia atrás\n",
    "d_2 = np.matmul(Theta2.T[1:, :], d_3) * ReLU_gradient(z_2)\n",
    "d_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices de cambio\n",
    "Construimos las matrices de cambio, para la actualización de las matrices de pesos sinápticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99323097, -0.07819653,  0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Delta3 = np.matmul(d_4, a_3.T)\n",
    "Delta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11955838, 0.        , 0.00098552, 0.02362263],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Delta2 = np.matmul(d_3, a_2.T)\n",
    "Delta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.02011474,  0.        ,  0.02011474],\n",
       "       [-0.00950799,  0.        , -0.00950799]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Delta1 = np.matmul(d_2, a_1.T)\n",
    "Delta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCosto(X, Y, Theta1, Theta2, Theta3):\n",
    "    \n",
    "    # Número de ejemplos\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Definir el costo del conjunto con el MSE\n",
    "    error = forwardProp(X.T, Theta1, Theta2, Theta3).T - Y\n",
    "    # Función de costo\n",
    "    J = np.mean(np.power(error, 2)) / 2\n",
    "    \n",
    "    # Cómputo de los gradientes \n",
    "    Delta1 = np.zeros_like(Theta1)\n",
    "    Delta2 = np.zeros_like(Theta2)\n",
    "    Delta3 = np.zeros_like(Theta3)\n",
    "    \n",
    "    # Backpropagation para cada ejemplo\n",
    "    for i in range(m):\n",
    "        # Obtenemos el ejemplo i\n",
    "        Xi = X[i, :].reshape(-1, 1)\n",
    "        # Propagamos hacia adelante el ejemplo\n",
    "        a_1 = addBias(Xi)\n",
    "        # Primera capa oculta\n",
    "        z_2 = np.matmul(Theta1, a_1)\n",
    "        a_2 = ReLU(z_2)\n",
    "        # Segunda capa oculta\n",
    "        a_2 = addBias(a_2)\n",
    "        z_3 = np.matmul(Theta2, a_2)\n",
    "        a_3 = ReLU(z_3)\n",
    "        # Capa de salida\n",
    "        a_3 = addBias(a_3)\n",
    "        z_4 = np.matmul(Theta3, a_3)\n",
    "        a_4 = linearAct(z_4)\n",
    "    \n",
    "        # Computamos el error en la capa de salida y propagamos hacia atrás\n",
    "        d_4 = a_4 - Ytest\n",
    "        d_3 = np.matmul(Theta3.T[1:], d_4) * ReLU_gradient(z_3)\n",
    "        d_2 = np.matmul(Theta2.T[1:, :], d_3) * ReLU_gradient(z_2)\n",
    "        \n",
    "        # Matrices de cambio\n",
    "        Delta3 += np.matmul(d_4, a_3.T)\n",
    "        Delta2 += np.matmul(d_3, a_2.T)\n",
    "        Delta1 += np.matmul(d_2, a_1.T)\n",
    "    \n",
    "    Theta1_grad = Delta1 / m\n",
    "    Theta2_grad = Delta2 / m\n",
    "    Theta3_grad = Delta3 / m\n",
    "\n",
    "    return J, Theta1_grad, Theta2_grad, Theta3_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2466899622269307, array([[ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.01005659,  0.0050279 ,  0.01005659],\n",
       "        [-0.00950884, -0.00475412, -0.00475362]]), array([[0.11956913, 0.        , 0.00040406, 0.02127887],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]), array([[-0.99332025, -0.07894058,  0.        ]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnCosto(Xtrain, Ytrain, Theta1, Theta2, Theta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Implementación del gradiente en descenso para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenarNN(X, Y, k1=3, k2=2, epochs=100, lr=0.001, statusRate=50):\n",
    "    \n",
    "    # Número de features\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # Matrices de pesos sinápticos aleatorias\n",
    "    Theta1 = np.random.normal(scale=0.05, size=(k1, n+1))\n",
    "    Theta2 = np.random.normal(scale=0.05, size=(k2, k1+1))\n",
    "    Theta3 = np.random.normal(scale=0.05, size=(1, 3))\n",
    "    \n",
    "    # Iterar sobre el número de epochs\n",
    "    for j in range(1, epochs+1):\n",
    "        # Obtener el costo y los gradientes\n",
    "        J, Theta1_grad, Theta2_grad, Theta3_grad = nnCosto(X, Y, Theta1, Theta2, Theta3)\n",
    "        \n",
    "        if (j % statusRate == 0):\n",
    "            print(\"Iteración: %d,\\tCosto: %0.4f\" % (j, J))\n",
    "        \n",
    "        # Actualizar los pesos sinápticos\n",
    "        Theta1 += -lr * Theta1_grad\n",
    "        Theta2 += -lr * Theta2_grad\n",
    "        Theta3 += -lr * Theta3_grad\n",
    "\n",
    "    return Theta1, Theta2, Theta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración: 500,\tCosto: 0.2093\n",
      "Iteración: 1000,\tCosto: 0.1920\n",
      "Iteración: 1500,\tCosto: 0.1774\n",
      "Iteración: 2000,\tCosto: 0.1651\n",
      "Iteración: 2500,\tCosto: 0.1550\n",
      "Iteración: 3000,\tCosto: 0.1467\n",
      "Iteración: 3500,\tCosto: 0.1401\n",
      "Iteración: 4000,\tCosto: 0.1349\n",
      "Iteración: 4500,\tCosto: 0.1310\n",
      "Iteración: 5000,\tCosto: 0.1282\n"
     ]
    }
   ],
   "source": [
    "Theta1_trained, Theta2_trained, Theta3_trained = entrenarNN(Xtrain, Ytrain, \n",
    "                                                            epochs=5000, lr=0.0001, statusRate=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4203282 , 0.42030112, 0.42028216, 0.42027073]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardProp(Xtrain.T, Theta1_trained, Theta2_trained, Theta3_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07737857,  0.07665175,  0.00062168],\n",
       "       [-0.00348409, -0.01284265, -0.08364031],\n",
       "       [ 0.11100635, -0.08891323, -0.05230451]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta1_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0038808 , -0.00094582, -0.03208755,  0.00058049],\n",
       "       [ 0.0434694 ,  0.05075939,  0.00527225, -0.02019826]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta2_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42138498,  0.04236947, -0.02563311]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta3_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de la segunda capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar los resultados de las dos neuronas de la segunda \n",
    "# capa oculta y de la salida\n",
    "def hiddenVizPlot(X, Theta1, Theta2, Theta3):\n",
    "    # Función para graficar cada una de las neuronas\n",
    "    def graficarNeurona(x1, x2, y):\n",
    "        f = np.abs(y-1) < 0.4\n",
    "        nf = np.logical_not(f)\n",
    "\n",
    "        plt.scatter(x1[f], x2[f], marker='*', label='1')\n",
    "        plt.scatter(x1[nf], x2[nf], marker='o', label='0')\n",
    "        plt.legend()\n",
    "        \n",
    "    # Obtener salidas de la capa oculta y final\n",
    "    a_3, a_4 = forwardProp(X.T, \n",
    "           Theta1, Theta2, Theta3, \n",
    "           retHidden=True)\n",
    "    \n",
    "    # Gráfica de las neuronas de la capa intermedia y salida\n",
    "    x1 = X.T[0]\n",
    "    x2 = X.T[1]\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.subplot(1,3,1)\n",
    "    graficarNeurona(x1, x2, a_3[0])\n",
    "    plt.title(\"a_3_1\")\n",
    "    plt.subplot(1,3,2)\n",
    "    graficarNeurona(x1, x2, a_3[1])\n",
    "    plt.title(\"a_3_2\")\n",
    "    plt.subplot(1,3,3)\n",
    "    graficarNeurona(x1, x2, a_4[0])\n",
    "    plt.title(\"a_4\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hiddenVizPlot(Xtrain, Theta1_trained, Theta2_trained, Theta3_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Conclusiones\n",
    "\n",
    "Al implementar el algoritmo de *backpropagation* se obtienen las derivadas parciales de cada una de las ponderaciones en la red neuronal de 2 capas ocultas y una de salida. Esto permite minimizar la función de costos utilizando el algoritmo de *gradient descent*. Sin embargo, los resultados indican que la función de costo utilizada (error cuadrático medio) en conjunto con la función de activación lineal de salida, no funcionan bien para el problema de la red de aprender la función XOR. Posiblemente, el proceso de entrenamiento sería más efectivo utilizando una salida sigmoidal y una función de costo de entropía cruzada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
